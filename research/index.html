<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="/assets/ad.png" type="image/x-icon" />
  <link href="https://fonts.googleapis.com/css2?family=Vollkorn:wght@400;600&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:wght@400;500;600&display=swap" rel="stylesheet">
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="/css/style.css">
  <title>research</title>
  <style>
    h1 {
      font-family: "EB Garamond", "adobe-caslon-pro", "Adobe Caslon Pro", "AdobeCaslonPro-Regular", "Big Caslon", "Adobe Caslon", "Caslon", "Libre Caslon Text", Georgia, "Times New Roman", serif;
      font-weight: bold;
      margin-bottom: 0.3em;
    }
    
    .menu {
      margin-bottom: 1.5em;
      margin-top: 0;
      text-align: left;
      font-size: 1rem;
    }
    
    .menu p {
      margin: 0;
      text-align: left;
    }
    
    .menu a {
      text-decoration: none;
      margin-right: 1em;
      display: inline-block;
      transition: transform 0.15s ease;
    }
    .menu a:hover {
      transform: skewX(-8deg);
    }
    
    .menu a:last-child {
      margin-right: 0;
    }

    .research-text {
      margin-bottom: -2em;
    }

    .research-text+.research-text {
      margin-top: 0.5em;
    }

    .project-section {
      margin-bottom: 2.5em;
      padding-bottom: 2em;
      border-bottom: 1px solid #e0e0e0;
    }

    .project-section:last-of-type {
      border-bottom: none;
    }
    
    /* Prevent layout shift on hover by using transform instead of font-style */
    a[style*="color:darkred"] {
      display: inline-block;
      transition: transform 0.15s ease;
    }
    a[style*="color:darkred"]:hover {
      transform: skewX(-8deg);
    }
  </style>
</head>

<body>
  <div id="contentContainer">

    <h1>research</h1>
    <!-- <hr> -->

    <div class="menu">
      <p>
        <a href="/about/">home</a>
        <a href="/research/">research</a>
        <a href="/teaching/">teaching</a>
        <a href="/notes/">notes</a>
        <a href="/blog/">blog</a>
        <a href="/code/">code</a>
        <a href="/cv/cv.pdf">vitae</a>
      </p>
    </div>

    <div class="profile-section">

      <div class="research-text">
        <p>
          I currently work in efficient linear algebra and theoretical neuroscience. I hope to
          contribute to the <a
            href="https://x.com/pfau/status/1974177288322494709" class="katex-title"
            style="font-size:15px; font-weight:400; text-decoration:none; color:darkred;">physics of learning</a>, a theory of natural and artificial intelligence.
        </p </div>

        <div class="research-text">
          <p>
            <b><a href="https://lpna.cs.illinois.edu/" class="katex-title" style="font-size:inherit; font-weight:400; text-decoration:none; color:black;"
                target="_blank">Lab for Parallel Numerical Algorithms</a></b>
          </p>

          <div class="project-section">
            <div style="margin-top: 1em;">
              <img src="/assets/relative-error-beta-sweep.png" alt="Relative error vs beta"
                style="float: left; width: 200px; height: auto; border: 1px solid #ddd; margin-right: 2em; margin-bottom: 1em;">
              <h3 style="margin-top: 0; font-size: 1.1em; font-weight: 400;">(i) mcmc for tensor network contractions
              </h3>
              <p style="margin-bottom: 0.8em;">
                We are developing a Markov chain Monte Carlo algorithm to estimate contractions of closed tensor
                networks. This allows efficient approximation of contraction quantities using methods from statistical
                physics to improve mixing. Such contractions \[
                \text{Tr}(ABCD) = \sum_{ijkl} A_{ij}B_{jk}C_{kl}D_{li}
                \]are trivial in small cases but become \(\#\textsf{P}\)-hard for large networks. Our approach targets
                use cases in quantum circuits and chemistry, where exact contraction is intractable.
              </p>
              <p style="margin-bottom: 0.5em;"><b>QSim '25</b> <a href="/research/tensor-contraction/"
                  class="katex-title"
                  style="font-size:15px; font-weight:400; text-decoration:none; color:darkred;">, Read more
                  here!</a>
              </p>
              <div style="clear: both;"></div>
            </div>
          </div>

          <div class="project-section">
            <div>
              <img src="/assets/amdm_convergence_placeholder.png" alt="AMDM algorithm"
                style="float: left; width: 200px; height: auto; border: 1px solid #ddd; margin-right: 2em; margin-bottom: 1em;">
              <h3 style="margin-top: 0; font-size: 1.1em; font-weight: 400;">(ii) bayesian tensor decompositions</h3>
              <p style="margin-bottom: 0.8em;">
                We are developing Bayesian algorithms for tensor decomposition and probabilistic linear algebra.
                Currently, we are building on Alternating Mahalanobis Distance Minimization (AMDM) to design a Bayesian
                tensor decomposition method with mode-wise covariance modeling. By interpreting AMDM as maximum
                likelihood estimation under a Kronecker-structured Gaussian prior, we aim to create new optimization
                schemes that combine statistical inference with deterministic tensor solvers for improved conditioning
                and uncertainty estimation.
              </p>
              <p style="margin-bottom: 0.5em;">More updates soon!</p>
              <div style="clear: both;"></div>
            </div>
          </div>
        </div>
      </div>


      <!-- <div class="bio-text">
      <p> 
        <b><a href="https://conelab.beckman.illinois.edu/" class="katex-title" style="font-size:inherit; font-weight:500;" target="_blank">Computation & Neurodynamics Lab</a></b> <br>
        I work on extracting analytic Floquet decompositions of periodic dynamical systems using symbolic AI. Given a linear time-periodic system, we seek closed-form expressions for its monodromy matrix and periodic transformation \(P(t)\) so that the dynamics can be represented in a time-invariant form. This enables exact optimal control design in transformed coordinates without relying on purely numerical Floquet computations.
      </p>
      <p>
        The systems of interest take the form
        \[
          \dot{x}(t) = A(t) x(t), \quad A(t+T) = A(t)
        \]
        where \(T\) is the fundamental period. Floquet theory guarantees a decomposition
        \[
          \Phi(t) = P(t) e^{R t}
        \]
        of the fundamental solution \(\Phi(t)\) into a \(T\)-periodic matrix \(P(t)\) and a constant matrix \(R\) containing the Floquet exponents. The monodromy matrix \(M = \Phi(T)\) satisfies \(M = P(0) e^{R T}\), linking periodic dynamics to their time-invariant equivalent.
      </p>
      <p>
        We compute numerical decompositions for benchmark systems, then fit symbolic models for \(P(t)\) and \(R\) using sparse regression and periodicity constraints. The learned analytic forms are validated by reconstructing \(\Phi(t)\) and comparing to direct numerical integration. Once obtained, these symbolic decompositions allow the design of optimal controllers via periodic Riccati equations, yielding closed-form feedback laws in the transformed coordinates.
      </p>
      <p>
        Current work focuses on defining appropriate function bases for symbolic fitting, ensuring periodicity during model discovery, and testing the framework on canonical periodic systems such as the Mathieu equation and parametrically forced oscillators.
      </p>
      More updates soon!
    </div> -->

    </div>


    <!-- <h2>Other</h2>

    <p>
        Materials from past applications and proposals can be found <a href="#">here</a>.
    </p> -->

  </div>
</body>

</html>


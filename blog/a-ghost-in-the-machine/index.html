<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' name='viewport' />
    <link rel="icon" href="/assets/ad.png" type="image/x-icon" />
    <title>a ghost in the machine — aniket deshpande</title>
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            box-sizing: border-box;
        }

        ::selection {
            background: #f9e6e6;
        }

        ::-moz-selection {
            background: #f9e6e6;
        }

        html {
            overflow-x: hidden;
            overflow-y: scroll;
        }

        body {
            width: 100%;
            margin: 25px 0;
            padding: 0;
            background: #fff;
            font-family: Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
            font-size: 16px;
            font-weight: 400;
            line-height: 1.5;
        }

        a {
            color: darkred;
            text-decoration: underline;
        }

        a:hover {
            color: #8B0000;
        }

        .container {
            max-width: 680px;
            margin: 0 auto;
            padding: 0 24px;
        }

        @media screen and (min-width: 600px) {
            body {
                margin: 50px 0 25px;
            }
            .container {
                padding: 0 24px;
            }
        }

        @media screen and (min-width: 800px) {
            body {
                margin: 120px 0 50px;
            }
            .container {
                padding: 0 24px;
            }
        }

        .bio {
            max-width: none;
        }

        .bio__header {
            margin-bottom: 20px;
            font-weight: 590;
            font-size: 21px;
        }

        .bio__header a {
            text-decoration: none;
            color: inherit;
        }

        .bio__header a:hover {
            text-decoration: none;
        }

        .bio__tabs {
            margin-bottom: 20px;
            font-style: italic;
        }

        .bio__tabs a {
            text-decoration: none;
            color: inherit;
        }

        .bio__tabs a:hover {
            color: darkred;
        }

        .content {
            width: 100%;
            max-width: none;
        }

        .post-title {
            font-weight: 700;
            font-size: 18px;
            margin-bottom: 5px;
        }

        .post-date {
            color: #666;
            font-style: italic;
            margin-bottom: 20px;
        }

        .post-body p {
            margin-bottom: 20px;
        }

        .callout {
            text-align: center;
            margin: 25px 0;
            background-color: #f8f8f8;
            padding: 15px;
            border-radius: 4px;
            font-style: italic;
        }

        figure {
            margin: 25px 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
        }

        figure.two-column > div {
            display: flex;
            gap: 1rem;
            justify-content: center;
            align-items: flex-start;
        }

        figure.two-column img {
            max-width: 48%;
            flex: 1;
        }

        @media (max-width: 768px) {
            figure.two-column > div {
                flex-direction: column;
            }
            figure.two-column img {
                max-width: 100%;
            }
        }

        figcaption {
            font-size: 15px;
            color: #666;
            margin-top: 10px;
            line-height: 1.4;
            font-style: italic;
        }

        .sidebar {
            background-color: transparent;
            border: none;
            padding: 0;
            margin: 20px 0;
            font-size: 14px;
            line-height: 1.5;
            color: #333;
        }

        .sidebar-title {
            font-weight: 400;
            margin-bottom: 4px;
            color: #333;
        }

        .sidebar-ref {
            display: inline-block;
            font-size: 0.75em;
            vertical-align: super;
            color: #8B0000;
            font-weight: 600;
            margin-left: 2px;
            text-decoration: none;
        }

        .sidebar-number {
            color: #8B0000;
            font-weight: 600;
            margin-right: 4px;
        }

        @media screen and (min-width: 1100px) {
            .post-body {
                position: relative;
            }

            .sidebar {
                position: absolute;
                left: calc(100% + 30px);
                width: 240px;
                margin: 0;
                font-size: 13px;
                line-height: 1.45;
            }
        }

        .back-link {
            margin-top: 40px;
            font-style: italic;
        }

        .back-link a {
            text-decoration: none;
        }

        .back-link a:hover {
            text-decoration: underline;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="bio">
            <div class="bio__header">
                <a href="/">a ghost in the machine</a>
            </div>
            <div class="bio__tabs">
                <a href="/about/">about</a> <span class="dot">·</span>
                <a href="/research/">research</a> <span class="dot">·</span>
                <a href="/teaching/">teaching</a> <span class="dot">·</span>
                <a href="/notes/">notes</a> <span class="dot">·</span>
                <a href="/blog/">blog</a> <span class="dot">·</span>
                <a href="/code/">code</a> <span class="dot">·</span>
                <a href="/cv/cv.pdf">vitae</a>
            </div>
        </div>

        <div class="content">
            <p class="post-date">12.28.2025</p>

            <div class="post-body">
                <p>
                    This post outlines a project I completed for a <a href="https://courses.grainger.illinois.edu/ECE598RE/fa2025/">graduate course</a> on dynamical systems, neural networks, and theoretical neuroscience. I explore how low-rank structure hidden in high-dimensional random connectivity gives rise to <i>spectral outliers:</i> isolated eigenvalues that dominate the macroscopic behavior of otherwise chaotic networks, and how these outliers can serve as early-warning signals for bifurcations in driven recurrent networks. The investigation blends random matrix theory, dynamical mean-field theory, and spectral analysis to make a conceptual ghost quantitative.
                </p>

                <p>
                    It's natural to now step back and ask: <i>what do we already know about chaotic RNNs? How does structure change the story?</i>
                </p>

                <p>
                    The most relevant recent work here is by Rainer Engelken and collaborators. Their <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e6f29fb27bb400f89f5584c175005679-Abstract-Conference.html">NeurIPS 2022</a> paper developed what they call a non-stationary dynamic mean-field theory (DMFT). Classical DMFT provides us with a way to describe the statistics of large random networks at stationary, but once the network is driven by time-varying inputs, the stationar assumption breaks. Engelken et al. derived a <i>time-resolved closure</i>: equations for the population mean activity, the two-time correlation function of fluctuations, and an auxiliary kernel that makes the system self-consistent. Practically, the framework gives us a tractable way to predict the network's response spectrum, its largest Lyapunov exponent, and even the information-theoretic capacity of the population to transmit signals.
                </p>

                <p>
                    We study a continuous-time rate network with \(N\) units. Each unit carries a synaptic current \(h_i(t)\) and fires at rate \(\phi(h_i)\), with \(\phi(x) = \max(0, x)\) (ReLU). The dynamics are given by the differential equation
                    \[
                    \tau \frac{d h_i}{dt} = -h_i + \sum_{j=1}^N J_{ij} \phi(h_j) + b I(t) + \xi_i(t).
                    \]
                    Here, \(\tau > 0\) is a membrane/leak timescale, \(J \in \mathbb{R}^{N \times N}\) is a connectivity matrix with balanced mean and random heterogeneity:
                    \[
                    J_{ij} = -\frac{b}{N}J_0 + \tilde{J}_{ij},\quad \tilde{J}_{ij} \sim \mathcal{N}(0, g^2/N).
                    \]
                    \(b > 0\) tunes the <i>tightness of balance</i><span class="sidebar-ref">[1]</span> (a stronger \(b\) means stronger recurrent inhibition), \(I(t)\) is a common input (e.g. an OU process with correlation time \(\tau_S\)), and \(\xi_i(t)\) is independent Gaussian noise such that \(\langle \xi_i(t) \xi_i(t') \rangle = \sigma^2 \tau \delta(t - t')\).

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[1]</span>Tightness of balance</div>
                    Larger \(b\) means excitation and inhibition cancel more precisely in the mean. This "tight balance" confines chaos to high-dimensional fluctuations while the mean mode stays clean and fast.
                </div>

                We decompose the currents into a <i>population mean</i> and <i>fluctuations</i>:
                    \[
                    h_i(t) = m(t) + \tilde{h}_i(t),\quad m(t) = \frac{1}{N} \sum_{i=1}^N h_i(t),\quad \langle \tilde{h}_i(t) \rangle = 0.
                    \]
                    Substituting this into the dynamics and using the mean of \(J\) yields two coupled equations 
                    \begin{align}
                        \tau \frac{d m}{dt} &= -m -b J_0 \nu(t) + b I(t) \\
                        \tau \frac{d \tilde{h}_i}{dt} &= -\tilde{h}_i + \sum_{j=1}^N \tilde{J}_{ij} \phi \left(m(t) + \tilde{h}_{ij}(t)\right) + \tilde{\xi}_i(t)
                    \end{align}
                    with the population rate \(\nu(t) = \frac{1}{N} \sum_{i=1}^N \phi\left(m(t) + \tilde{h}_i(t)\right)\). 
                    Solving the first equation for \(\nu\) gives the <i>balance equation</i>
                    \[
                    \nu(t) = \frac{1}{J_0}I(t) - \frac{1}{b J_0} \left(\tau \frac{d m}{dt} + m\right).
                    \]
                    In the tight-balance limit \(b \to \infty\), \(\nu(t)\approx I(t)/J_0\) (linear tracking) and the mean mode acquires an effective timescale \(\tau_{\mathrm{eff}} = \tau/b\). This leads to the first core result.
                </p>
                    
                <div class="callout">
                    "Balance speeds up the population mean by a factor of \(b\)."
                </div>
                    
                <p>
                    So far, we've seen the spectral fingerprint: the rank-one mean term adds a single eigenvalue at \(-b\), while the random bulk remains a circular cloud of radius \(g\). This lone outlier is the ghost steering the mean activity. 
                    But spectra are only the beginning. The NeurIPS paper showed how this outlier governs the <i>dynamical timescale</i>  of the mean. Take a step input and watch the population mean \(m(t)\) relax. The prediction from the balance equation is relatively simple:
                    \[
                    \tau_{\mathrm{eff}} = \frac{\tau}{b}.
                    \]
                    As \(b\) increases, the mean mode tracks input \(b\)-times faster. In simulations, fitting the exponential decay of \(m(t)\) confirms that the timescale drops as \(1/b\). 
                </p>

                <p>
                    Faster mean dynamics suggest wider bandwidth for signal encoding. The DMFT closure makes this intuition quantitative. By treading residuals \(\tilde{h}(t)\) as a Gaussian process with covariance \(c(t, s)\), one can compute the input-output <i>coherence</i>
                    \[
                    C_{I\nu}(f) = \frac{\left| S_{I\nu}(f)\right|^2}{S_{II}(f) S_{\nu\nu}(f)},
                    \]
                    Here, \(S_{I\nu}\) is the cross-spectrum of the input and output, and \(S_{II}\), \(S_{\nu\nu}\) are their respective autospectra. The result is that higher balance values keep coherence close to one across higher frequencies, reflecting faster tracking and higher effective bandwidth.
                </p>

                <p>
                    Integrating this coherence function gives us a Gaussian-channel lower bound on the <i>mutual information</i> between the input and output signals.
                    \[
                    R_{\mathrm{lb}} = -\int\, df \log_2\left(1 - C_{I\nu}(f)\right).
                    \]
                </p>

                <p>
                    This is the one of the headline results of the paper, and our second core finding.
                </p>
                    
                <div class="callout">
                    "The information rate grows nearly linearly with balance until it saturates at the bandwidth limit imposed by the input itself."
                </div>

                <p>
                    To complete our understanding of this paper, it's worth highlighting a key result from <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010590">a complementary paper</a> done by Engelken et al. This PLOS paper provides code that shows the same non-stationary DMFT framework was used to compute the largest Lyapunov exponent<span class="sidebar-ref">[2]</span> \(\lambda_1\).

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[2]</span>Lyapunov exponents</div>
                    The largest Lyapunov exponent \(\lambda_1\) measures how fast nearby trajectories diverge exponentially. Positive means chaos, negative means stability, zero marks the bifurcation boundary.
                </div>
                </p>

                <p>
                    The surprising finding was that common input is poor at surpressing chaos. Because of balance, the common drive enters the mean equation and is canceled by recurrent inhibition. As a result, \(\lambda_1\) only turns negative at very large input amplitudes. Independent input, on the other hand, enters the fluctuation channel directly and is not canceled, so even small amplitudes can suppress chaos.
                </p>
                    
                <p>
                    This closes the loop on what we know. Balance plants a real outlier eigenvalue at \(-b\), accelerates the mean mode, broadens encoding bandwidth, and shapes how chaos can be tamed. But the story doesn't end with theory. The natural next question is experimental: <i>can we go beyond balance and use low-rank structure to predict when a driven network crosses the stability boundary?</i>
                </p>

                <p>
                    Before pushing into new territory, I wanted to confirm the baseline machinery by reproducing the spectral and DMFT fingerprints under driven chaos. There are three sanity checks:
                </p>

                <p>
                    (i) Trajectories behave as expected as \(g\) increases: fluctuations grow, but remain bounded under strong balance.<br>
                    (ii) Non-stationary DMFT qualitatively matches the time-resolved mean \(m(t)\) and variance \(c(t,t)\).<br>
                    (iii) The spectrum of \(J\) is exactly what it should be: a Ginibre disk of radius \(\approx g\) plus the real outlier at \(-b\).
                </p>

                <figure>
                    <img src="/assets/post4/fig1-trajectories.png" alt="raw currents h_i(t) and population mean m(t) in low-g vs high-g regimes under OU drive">
                    <figcaption>
                        <b>Figure 1.</b> Raw trajectories \(h_i(t)\) (a few neurons) overlaid with the population mean \(m(t)\), comparing a low-g and high-g regime under OU drive.
                    </figcaption>
                </figure>

                <figure>
                    <img src="/assets/post4/fig2-ns-dmft.png" alt="non-stationary DMFT vs simulation: mean m(t) and variance c(t,t)">
                    <figcaption>
                        <b>Figure 2.</b> Non-stationary DMFT (time-resolved closure) vs network simulation: the population mean \(m(t)\) and variance \(c(t,t)\) track qualitatively, which is all we need before asking spectral questions.
                    </figcaption>
                </figure>

                <p>
                    The spectral check is the one I care about most, because everything that follows is an outlier story. If balance is doing what it claims, then \(J\) should look like: circular bulk + one real spike at \(-b\). And that's exactly what emerges.
                </p>

                <figure class="two-column">
                    <div>
                        <img src="/assets/post4/fig3a.png" alt="spectrum of J: Ginibre bulk with outlier at -b">
                        <img src="/assets/post4/fig3b.png" alt="largest Lyapunov exponent as function of g">
                    </div>
                    <figcaption>
                        <b>Figure 3.</b> (Left) Spectrum of \(J\): Ginibre bulk of radius \(\approx g\) plus the balanced outlier at \(-b\). (Right) Largest Lyapunov exponent \(\lambda_1(g)\) under OU drive, approaching the transition as \(g\) increases (but staying negative here).
                    </figcaption>
                </figure>

                <p>
                    This already hints at something subtle. We often talk as if "the transition to chaos" is a crisp curve in parameter space. But at finite \(N\) and finite simulation horizon, the world is softer: \(\lambda_1\) trends toward zero without crossing. That softness matters once we start using spectral proxies.
                </p>

                <p>
                    The real object controlling stability in a driven system is not the spectrum of \(J\), but the linearization along the trajectory. To move from connectivity outliers to Jacobian outliers, linearize the dynamics:
                    \[
                    A(t) = -I + J D(t), \qquad D(t) = \mathrm{diag}\!\left(\phi'(h(t))\right),
                    \]
                    where \(D(t)\) is the <i>gain mask</i> that encodes which neurons are active at time \(t\) under ReLU.
                </p>

                <p>
                    If the drive is slow enough, a natural (dangerous) idea is to average:
                    \[
                    A_{\mathrm{avg}} = -I + J \bar D, \qquad \bar D \approx \mathbb{E}_t[D(t)].
                    \]
                    This is not a theorem. It's a controlled hallucination: "pretend the driven Jacobian is approximately stationary." But it's the right hallucination if you want an outlier story.
                </p>

                <div class="callout">
                    "In a non-autonomous network, the outlier you want is not in \(J\). It's in the <i>filtered</i> matrix \(J\bar D\)."
                </div>

                <p>
                    Now we move to a rank-one experiment with a surprisingly useful proxy. Add a rank-one perturbation:
                    \[
                    J = gW - \frac{b}{N}\mathbf{1}\mathbf{1}^\top + m\,u v^\top,
                    \]
                    with \(u,v\) random unit vectors chosen orthogonal to \(\mathbf{1}\) (so we don't just re-inject another mean mode). The knob is \(m\), the low-rank strength.
                </p>

                <p>
                    The experiment loop is:<br>
                    (1) Simulate the OU-driven network at fixed \((g,b)\) while sweeping \(m\);<br>
                    (2) Build \(A_{\mathrm{avg}}\) from the time-averaged gain mask \(\bar D\);<br>
                    (3) Extract the eigenvalue of \(A_{\mathrm{avg}}\) with largest real part (the "outlier");<br>
                    (4) Compute \(\lambda_1\) using the Benettin algorithm<span class="sidebar-ref">[3]</span> along the full time-dependent Jacobian.

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[3]</span>Benettin algorithm</div>
                    The Benettin algorithm computes Lyapunov exponents by tracking orthonormalized perturbation vectors along the trajectory, avoiding exponential overflow that would occur with naive methods.
                </div>
                </p>

                <p>
                    Here is the punchline: for rank-one structure, there's a scalar proxy for the low-rank outlier:
                    \[
                    \hat\lambda_{\mathrm{out}}(m) \;\approx\; m\,\bar\alpha - 1,
                    \qquad \bar\alpha = v^\top (\bar D u).
                    \]
                    This is the smallest possible "low-dimensional stability indicator": one number extracted from \(\bar D\). If \(\hat\lambda_{\mathrm{out}}\) tracks \(\lambda_1\), then we've learned something structural: the low-rank mode is steering the boundary.
                </p>

                <figure>
                    <img src="/assets/post4/fig4-low_rank_outlier.png" alt="spectrum of A_avg as m increases; proxy vs empirical outlier real part; LLE vs m">
                    <figcaption>
                        <b>Figure 4.</b> Low-rank outlier proxy vs largest Lyapunov exponent. (a) Spectrum of \(A_{\mathrm{avg}}\) as \(m\) increases: a single real outlier drifts right. (b) Proxy \(m(v^\top \bar D u)-1\) compared to the empirical \(\Re \lambda_{\mathrm{out}}(A_{\mathrm{avg}})\). (c) \(\lambda_1(m)\) from Benettin along the full time-dependent Jacobian.
                    </figcaption>
                </figure>

                <p>
                    What I find satisfying is not that the curves coincide. They don't. What matters is that they <i>move together</i>. As \(m\) increases, the outlier of \(A_{\mathrm{avg}}\) slides right; the scalar proxy slides with it; and \(\lambda_1(m)\) trends toward zero. In the parameter range I explored, everything stayed stable (no actual crossing), but the alignment is the central sanity check:
                </p>

                <p>
                    <i>The low-rank mode appears to be the first macroscopic "handle" on stability, even under drive.</i>
                </p>

                <div class="callout">
                    "\(\lambda_1\) is the verdict. The outlier proxy is an early-warning signal."
                </div>

                <p>
                    There is a clean reason to be suspicious about where the proxy fails, and why that's not a defeat. Stability in a driven system is governed by the time-ordered product of Jacobians, a non-commutative object. A single averaged matrix \(A_{\mathrm{avg}}\) cannot capture non-normal transient growth<span class="sidebar-ref">[4]</span>, nor can it see fast input regimes. This is why, in exploratory phase-slices at larger \(g\) and \(m\), the proxy can become positive while sparse \(\lambda_1\) checks remain negative.

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[4]</span>Non-normal growth</div>
                    Non-normal matrices can amplify perturbations transiently even when all eigenvalues are stable. The time-ordered product of Jacobians is non-commutative, so eigenvalues of \(A_{\mathrm{avg}}\) miss this transient amplification.
                </div>
                </p>

                <p>
                    I don't see this as failure. I see it as a map: it tells us exactly where the quasi-stationary picture breaks.
                </p>

                <p>
                    The honest endgame for making the ghost quantitative is not a proxy. It's a full low-rank, non-stationary DMFT: a time-resolved closure not just for the mean \(m(t)\) and covariance \(c(t,s)\), but also for the <i>overlap variables</i> \(\kappa(t)\) that carry low-rank structure. At rank \(R\), these overlaps live in \(\mathbb{R}^R\), and they are the natural "order parameters" of the structured mode.
                </p>

                <p>
                    Once you have \((m,c,\kappa)\), the stability question becomes low-dimensional again: linearize the overlap update map and track when the maximal real part crosses zero. That would turn the ghost into a genuine stability boundary, not a heuristic.
                </p>

                <p>
                    The path forward is clear. Solve the low-rank NS-DMFT equations numerically (including overlaps) instead of relying on trajectory averages. Map the \((g,m)\) plane at larger \(N\) and longer horizons until \(\lambda_1\) actually crosses zero. Move beyond rank-one: \(R>1\) structure introduces multiple outliers, and the question becomes <i>which</i> one controls the bifurcation. Stress test the assumptions: rapid drive, large \(g\), non-normality, different nonlinearities.
                </p>

                <p>
                    What began as a study of spectral outliers in balanced networks ended in a concrete bridge between finite-rank matrix theory and the stability of non-autonomous dynamical systems. The ghost is no longer just a metaphor. It's a quantifiable signal, a low-dimensional order parameter hidden inside high-dimensional chaos. I'm grateful to Rainer Engelken for his guidance throughout this project; the full technical report is available <a href="/assets/post4/low-rank-rnns.pdf">here</a>.
                </p>

                <div class="back-link">
                    <a href="/blog/">← back to blog</a>
                </div>
            </div>
        </div>
    </div>
</body>
</html>

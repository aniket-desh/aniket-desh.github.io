<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' name='viewport' />
    <link rel="icon" href="/assets/ad.png" type="image/x-icon" />
    <title>the slow art of fast mixing — aniket deshpande</title>
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            box-sizing: border-box;
        }

        ::selection {
            background: #f9e6e6;
        }

        ::-moz-selection {
            background: #f9e6e6;
        }

        html {
            overflow-x: hidden;
            overflow-y: scroll;
        }

        body {
            width: 100%;
            margin: 25px 0;
            padding: 0;
            background: #fff;
            font-family: Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
            font-size: 16px;
            font-weight: 400;
            line-height: 1.5;
        }

        a {
            color: darkred;
            text-decoration: underline;
        }

        a:hover {
            color: #8B0000;
        }

        .container {
            max-width: 1200px;
            margin: 0 30px;
        }

        @media screen and (min-width: 600px) {
            body {
                margin: 50px 0 25px;
            }
            .container {
                margin: 0 60px;
            }
        }

        @media screen and (min-width: 800px) {
            body {
                margin: 120px 0 50px;
            }
            .container {
                margin: 0 140px;
            }
        }

        .bio {
            max-width: 500px;
        }

        .bio__header {
            margin-bottom: 20px;
            font-weight: 590;
            font-size: 21px;
        }

        .bio__header a {
            text-decoration: none;
            color: inherit;
        }

        .bio__header a:hover {
            text-decoration: none;
        }

        .bio__tabs {
            margin-bottom: 20px;
            font-style: italic;
        }

        .bio__tabs a {
            text-decoration: none;
            color: inherit;
        }

        .bio__tabs a:hover {
            color: darkred;
        }

        .content {
            width: 81%;
            max-width: 970px;
        }

        @media screen and (max-width: 800px) {
            .content {
                width: 100%;
            }
        }

        .post-title {
            font-weight: 700;
            font-size: 18px;
            margin-bottom: 5px;
        }

        .post-date {
            color: #666;
            font-style: italic;
            margin-bottom: 20px;
        }

        .post-body p {
            margin-bottom: 20px;
        }

        .post-body h3 {
            font-weight: 700;
            font-size: 18px;
            margin: 25px 0 10px 0;
        }

        .post-body ul {
            margin: 0 0 20px 0;
            padding-left: 0;
            list-style: none;
        }

        .post-body ul li {
            margin-bottom: 15px;
        }

        .sidebar {
            background-color: #f8f8f8;
            border-left: 3px solid #8B0000;
            padding: 12px 15px;
            margin: 20px 0;
            font-size: 15px;
            line-height: 1.5;
        }

        .sidebar-title {
            font-weight: 700;
            margin-bottom: 8px;
            color: #333;
        }

        .sidebar-ref {
            display: inline-block;
            font-size: 0.75em;
            vertical-align: super;
            color: #8B0000;
            font-weight: 700;
            margin-left: 2px;
            text-decoration: none;
        }

        .sidebar-number {
            color: #8B0000;
            font-weight: 700;
            margin-right: 4px;
        }

        @media screen and (min-width: 1200px) {
            .post-body {
                position: relative;
            }

            .sidebar {
                position: absolute;
                left: calc(100% + 40px);
                width: 280px;
                margin: 0;
                font-size: 14px;
            }
        }

        .back-link {
            margin-top: 40px;
            font-style: italic;
        }

        .back-link a {
            text-decoration: none;
        }

        .back-link a:hover {
            text-decoration: underline;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="bio">
            <div class="bio__header">
                <a href="/">the slow art of fast mixing</a>
            </div>
            <div class="bio__tabs">
                <a href="/about/">about</a> <span class="dot">·</span>
                <a href="/research/">research</a> <span class="dot">·</span>
                <a href="/teaching/">teaching</a> <span class="dot">·</span>
                <a href="/notes/">notes</a> <span class="dot">·</span>
                <a href="/blog/">blog</a> <span class="dot">·</span>
                <a href="/code/">code</a> <span class="dot">·</span>
                <a href="/cv/cv.pdf">vitae</a>
            </div>
        </div>

        <div class="content">
            <p class="post-date">07.25.2025</p>

            <div class="post-body">
                <p>
                    In my studies of quantum information and learning theory, I find that a particular computational tool has become the center of rigorous mathematical research: sampling. This post walks through the rich theory of MCMC sampling and the results that allow Monte Carlo simulations to be numerical necessities in many fields.
                </p>

                <p>
                    In quantum information, Path-Integral Monte Carlo (PIMC) is a primary method for simulating thermal states of stoquastic quantum systems. PIMC transforms a quantum partition function into a classical Gibbs distribution by introducing an "imaginary-time" dimension, then applies Markov Chain Monte Carlo to sample from that distribution. Recently, these Markov chains were proven to mix <i>rapidly</i>, using the canonical path method. This rigorously justifies the use of PIMC in partition function estimation.
                </p>

                <p>
                    Suppose we hope to estimate the partition function of a \(1\)D stoquastic Hamiltonian
                    \[
                    H = \sum_{i=1}^{n-1}Z_{i}Z_{i+1} + \sum_{i=1}^{n}h_iX_i
                    \]
                    With a Suzuki-Trotter decomposition, this quantum system becomes a classical one in \(n \times \beta\) space-time, with a Gibbs distribution over paths \(x \in \{0, 1 \}^{n\beta}\). Sampling from this distribution using MCMC requires Glauber dynamics. However, our estimates can be wildly inaccurate unless we prove mixing time<span class="sidebar-ref">[1]</span> to be <i>small</i>.

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[1]</span>Mixing time</div>
                    The mixing time is defined as
                    \[
                    \tau(\epsilon) = \min\{t : \|\mu_t - \pi\|_{TV} \leq \epsilon\}
                    \]
                    where \(\mu_t\) is the distribution after \(t\) steps and \(\pi\) is the stationary distribution. It measures how many steps are needed to get \(\epsilon\)-close in total variation distance.
                </div>
                </p>

                <p>
                    In learning theory, specifically in probabilistic inference on graphical models, sampling-based algorithms (e.g. Gibbs sampling, importance sampling) are essential to approximating joint distributions when exact inference is intractable. Consider a Markov random field over binary variables \(X_1, \ldots, X_n\) with pairwise interactions:
                    \[
                    P(x) \propto \exp \left( \sum_{(i, j) \in E} \theta_{ij} x_i x_j + \sum_{i}\theta_i x_i \right)
                    \]
                    and we want to compute marginal probabilities \(P(X_i = 1)\). Exact inference is \(\mathsf{\# P}\)-hard, so we use Gibbs sampling. But how long does this take to converge<span class="sidebar-ref">[2]</span> to the true distribution?

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[2]</span>Total variation distance</div>
                    Total variation distance \(\|\mu - \pi\|_{TV} = \frac{1}{2}\sum_x |\mu(x) - \pi(x)|\) measures how far a distribution \(\mu\) is from the stationary distribution \(\pi\). It's the natural metric for convergence: at most 1, and 0 only when distributions are identical.
                </div>

                Once again, this depends on how fast the Markov chain mixes. Both of these examples illustrate that sampling is not just a numerical tool, but a computational lifeline in dire need of some theoretical justifications: <i>do these chains converge fast enough for meaningful inference?</i>
                </p>

                <p>
                    Canonical paths are a powerful combinatorial framework for proving rapid mixing in high-dimensional Markov chains. Let us define a set of paths \( \Gamma = \{ \gamma_{x, y} \}_{x, y \in \Omega} \) for each pair of distinct states in the state space \( \Omega \). The <i>edge congestion</i> is defined as
                    \[
                    \rho = \max_{e \in E} \frac{1}{Q(e)} \sum_{(x, y) : e \in \gamma_{x, y}} \pi(x)\pi(y)
                    \]
                    where \( Q(e) = \pi(u) P(u, v)\) is the ergodic flow through edge \(e_{u \to v}\).

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[3]</span>Conductance</div>
                    Conductance \(\Phi\) measures the bottleneck probability flow: how easily probability mass can move between large sets. Larger \(\Phi\) means fewer bottlenecks and faster mixing.
                </div>

                In 1989, Jerrum and Sinclair showed that one can lower-bound the <i>conductance</i><span class="sidebar-ref">[3]</span> \(\Phi\) and <i>spectral gap</i><span class="sidebar-ref">[4]</span> \(\lambda_1\) with
                    \[
                    \Phi \geq \frac{1}{2\rho},\quad \lambda_1 \geq \frac{1}{2\rho^2}
                    \]
                yielding a mixing-time bound (in total variation) of
                    \[
                    \tau(\epsilon) \leq \frac{2}{\Phi^2} \left(\ln \frac{1}{\pi_{min}} + \ln \frac{1}{\epsilon} \right).
                    \]

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[4]</span>Spectral gap</div>
                    The spectral gap \(\lambda_1 = 1 - |\lambda_2|\) measures the separation between the largest and second-largest eigenvalue of the transition matrix. Larger gap means exponentially faster convergence.
                </div>
                </p>

                <p>
                    This foundational result was the justification for their FPRAS<span class="sidebar-ref">[5]</span> algorithm for approximating the permanent of a matrix, and has since been generalized. However, canonical paths designate a single path to each pair of states potentially causing large congestion. So, we introduce <i>fractional flows</i>, a probability distribution over paths for each pair. Let \( f_{x, y}(\gamma)\) be the amount of flow along path \(\gamma\) from \(x\) to \(y\), subject to the constraint:
                    \[
                    \sum_{\gamma \in \Gamma} f_{x, y}(\gamma) = \pi(x) \pi(y).
                    \]

                <div class="sidebar">
                    <div class="sidebar-title"><span class="sidebar-number">[5]</span>FPRAS</div>
                    A Fully Polynomial Randomized Approximation Scheme is a randomized algorithm that computes a \((1 \pm \epsilon)\)-approximation in time polynomial in both the input size and \(1/\epsilon\).
                </div>

                The total load on an edge \(e\) and fractional congestion are defined as
                    \[
                    F(e) = \sum_{x, y}\sum_{\gamma \ni e} f_{x, y}(\gamma),\quad \rho_f = \max_{e \in E} \frac{F(e)}{Q(e)}.
                    \]
                </p>

                <p>
                    A vital insight in modern canonical path theory is that rapid mixing is still ensured when \(\rho_f\) is small. Fractional flows even often yield <i>smaller</i> congestion bounds than deterministic path assignments. Note that <i>path length matters.</i> Diaconis and Stroock showed that with a maximum path length \(l_{max} = \max_{x, y} \left| \gamma_{x, y} \right|\), the spectral gap satisfies the bound
                    \[
                    \lambda_1 \geq \frac{1}{\rho_f l_{max}}
                    \]
                    This bound tightens mixing time estimates by involving both congestion and how long it takes information to propogate through the graph structure. With long but sparse paths, it can be much stronger than the \(\rho^2\) bound.
                </p>

                <p>
                    In 2025, Chen et al. heavily improved on the Jerrum-Sinclair result by redefining the analysis for the perfect matching chain. For a graph with \(n\) vertices, \(m\) edges, and maximum degree \(\Delta\), they showed that the mixing time satisfies
                    \[
                    \tau(\epsilon) = \widetilde{\mathcal{O}} \left(\Delta^2 m + \Delta^2 \log\left(1/\epsilon\right) \right).
                    \]
                    In summary, the journey from Jerrum & Sinclair's classical framework to the modern refinement by Chen et al. illustrates the mathematical richness of sampling theory. For problems in quantum simulation and learning algorithms, where sampling is vital, this theory is mathematical assurance that Monte Carlo isn't just plausible, it's provably fast.
                </p>

                <h3>references</h3>
                <ul>
                    <li><strong>Chen et al. (2025)</strong> – <em>Faster Mixing of the Jerrum–Sinclair Chain</em>.<br>Shows the mixing time is \(\widetilde O(\Delta^2 m)\) for the perfect-matching chain. <a href="https://arxiv.org/abs/2504.02740" target="_blank">arxiv.org</a></li>

                    <li><strong>Guruswami (2016)</strong> – <em>Rapidly Mixing Markov Chains: A Comparison of Techniques</em>.<br>A helpful overview of canonical paths vs. fractional flows. <a href="https://arxiv.org/abs/1603.01512" target="_blank">arxiv.org</a></li>

                    <li><strong>Diaconis & Stroock (1991)</strong> – <em>Geometric Bounds for Eigenvalues of Markov Chains.</em><br>Proved that spectral gap is bounded by \(1 / (\rho_f \cdot l_{max})\), tying congestion to path length. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/a600cdf3a53f93bcb85cb37343a8d831-Paper-Conference.pdf" target="_blank">link</a></li>

                    <li><strong>Jerrum & Sinclair (1989)</strong> – <em>Approximating the Permanent</em>.<br>Introduced the use of Markov chains for approximating the permanent of a matrix. <a href="https://dl.acm.org/doi/10.1145/74334.74335" target="_blank">link</a></li>
                </ul>

                <div class="back-link">
                    <a href="/blog/">← back to blog</a>
                </div>
            </div>
        </div>
    </div>
</body>
</html>

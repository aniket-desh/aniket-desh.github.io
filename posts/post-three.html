<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css2?family=Vollkorn:wght@400;600&display=swap" rel="stylesheet">
    <link rel="icon" href="../assets/ad.png" type="image/x-icon" />
    <link rel="stylesheet" href="../src/style.css">
    <title>a ghost in the machine — aniket deshpande</title>
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        h1 {
            font-family: "adobe-caslon-pro", "Adobe Caslon Pro", "AdobeCaslonPro-Regular", "Big Caslon", "Adobe Caslon", "Caslon", "Libre Caslon Text", Georgia, "Times New Roman", serif;
        }
        h3 {
            font-family: "adobe-caslon-pro", "Adobe Caslon Pro", "AdobeCaslonPro-Regular", "Big Caslon", "Adobe Caslon", "Caslon", "Libre Caslon Text", Georgia, "Times New Roman", serif;
        }
        .centered-quote {
            text-align: center;
            margin-top: -1rem;
            margin: 2rem auto;
            max-width: 40ch;
            font-style: italic;
        }
    </style>
</head>

<body>
<div id="contentContainer">

        <div class="flex flex-col mt-8 text-sm">
        <a href="../src/blog.html">&larr; blog</a>
    </div>

    <div style="position: relative; margin-bottom: 1.5em;">
        <svg style="position: absolute; top: -20px; left: -30px; width: calc(100% + 60px); height: 90px; z-index: -1;" 
             viewBox="0 0 1000 90" preserveAspectRatio="none">
            <!-- dendrites -->
            <path d="M 155 45 Q 125 35 105 25 Q 85 20 65 15" stroke="#888888" stroke-width="1.2" fill="none" opacity="0.5"/>
            <path d="M 155 50 Q 125 55 105 60 Q 85 65 65 70" stroke="#888888" stroke-width="1.2" fill="none" opacity="0.5"/>
            <path d="M 155 40 Q 130 30 115 20" stroke="#888888" stroke-width="1" fill="none" opacity="0.5"/>
            <path d="M 155 55 Q 130 65 115 75" stroke="#888888" stroke-width="1" fill="none" opacity="0.5"/>
            
            <!-- soma -->
            <ellipse cx="180" cy="45" rx="25" ry="18" fill="none" stroke="#888888" stroke-width="1.5" opacity="0.6"/>

            <!-- axon hillock -->
            <path d="M 205 45 Q 220 45 235 45" stroke="#888888" stroke-width="2" fill="none" opacity="0.6"/>

            <!-- axon -->
            <path d="M 235 45 Q 300 40 400 42 Q 500 44 600 41 Q 700 38 800 40 Q 850 41 900 39" stroke="#888888" stroke-width="1.8" fill="none" opacity="0.6"/>

            <!-- axon terminals (synaptic endings) -->
            <path d="M 880 39 Q 890 35 900 30" stroke="#888888" stroke-width="1" fill="none" opacity="0.5"/>
            <path d="M 885 40 Q 895 44 905 48" stroke="#888888" stroke-width="1" fill="none" opacity="0.5"/>
            <path d="M 890 38 Q 900 42 910 45" stroke="#888888" stroke-width="1" fill="none" opacity="0.5"/>

            <!-- synaptic vesicles (small circles at terminals) -->
            <circle cx="895" cy="32" r="1.5" fill="#888888" opacity="0.4"/>
            <circle cx="900" cy="46" r="1.5" fill="#888888" opacity="0.4"/>
            <circle cx="905" cy="43" r="1.5" fill="#888888" opacity="0.4"/>
        </svg>
        <h1>a ghost in the machine</h1>
    </div>

    <p><span class="gray-date">09.28.2025</span><br><br>
        This post will be the first of \(N\) in a series outlining a project I am completing for a <a href="https://courses.grainger.illinois.edu/ECE598RE/fa2025/" class="katex-title"
            style="font-size:inherit; font-weight:500; text-decoration:underline; color:black; transition:color 0.3s;"
            onmouseover="this.style.color='darkred';" onmouseout="this.style.color='black';">graduate course</a>  on dynamical systems, neural networks, and theoretical neuroscience. Specifically, I will be exploring how low-rank structure hidden in high-dimensional random connectivity gives rise to <i>spectral outliers:</i> isolated eigenvalues that dominate the macroscopic behavior of otherwise chaotic networks. We will pull ideas from theories of random matrices, dynamical mean fields, and free probability to understand these phenomena. <br><br>

            It's natural to now step back and ask: <i>what do we already know about chaotic RNNs? How does structure change the story?</i> <br><br>

            The most relevant recent work here is by Rainer Engelken and collaborators. Their <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e6f29fb27bb400f89f5584c175005679-Abstract-Conference.html">NeurIPS 2022</a> paper developed what they call a non-stationary dynamic mean-field theory (DMFT). Classical DMFT provides us with a way to describe the statistics of large random networks at stationary, but once the network is driven by time-varying inputs, the stationar assumption breaks. Engelken et al. derived a <i>time-resolved closure</i>: equations for the population mean activity, the two-time correlation function of fluctuations, and an auxiliary kernel that makes the system self-consistent. Practically, the framework gives us a tractable way to predict the network's response spectrum, its largest Lyapunov exponent, and even the information-theoretic capacity of the population to transmit signals. <br><br>

            We study a continuous-time rate network with \(N\) units. Each unit carries a synaptic current \(h_i(t)\) and fires at rate \(\phi(h_i)\), with \(\phi(x) = \max(0, x)\) (ReLU). The dynamics are given by the differential equation
            \[
            \tau \frac{d h_i}{dt} = -h_i + \sum_{j=1}^N J_{ij} \phi(h_j) + b I(t) + \xi_i(t).
            \]
            Here, \(\tau > 0\) is a membrane/leak timescale, \(J \in \mathbb{R}^{N \times N}\) is a connectivity matrix with balanced mean and random heterogeneity:
            \[
            J_{ij} = -\frac{b}{N}J_0 + \tilde{J}_{ij},\quad \tilde{J}_{ij} \sim \mathcal{N}(0, g^2/N).
            \]
            \(b > 0\) tunes the <i>tightness of balance</i> (a stronger \(b\) means stronger recurrent inhibition), \(I(t)\) is a common input (e.g. an OU process with correlation time \(\tau_S\)), and \(\xi_i(t)\) is independent Gaussian noise such that \(\langle \xi_i(t) \xi_i(t') \rangle = \sigma^2 \tau \delta(t - t')\). We decompose the currents into a <i>population mean</i> and <i>fluctuations</i>:
            \[
            h_i(t) = m(t) + \tilde{h}_i(t),\quad m(t) = \frac{1}{N} \sum_{i=1}^N h_i(t),\quad \langle \tilde{h}_i(t) \rangle = 0.
            \]
            Substituting this into the dynamics and using the mean of \(J\) yields two coupled equations 
            \begin{align}
                \tau \frac{d m}{dt} &= -m -b J_0 \nu(t) + b I(t) \\
                \tau \frac{d \tilde{h}_i}{dt} &= -\tilde{h}_i + \sum_{j=1}^N \tilde{J}_{ij} \phi \left(m(t) + \tilde{h}_{ij}(t)\right) + \tilde{\xi}_i(t)
            \end{align}
            with the population rate \(\nu(t) = \frac{1}{N} \sum_{i=1}^N \phi\left(m(t) + \tilde{h}_i(t)\right)\). 
            Solving the first equation for \(\nu\) gives the <i>balance equation</i>
            \[
            \nu(t) = \frac{1}{J_0}I(t) - \frac{1}{b J_0} \left(\tau \frac{d m}{dt} + m\right).
            \]
            In the tight-balance limit \(b \to \infty\), \(\nu(t)\approx I(t)/J_0\) (linear tracking) and the mean mode acquires an effective timescale \(\tau_{\mathrm{eff}} = \tau/b\). This leads to the first core result.
            </p>
            
            <div style="text-align: center; margin: 0.5em 0 2em 0; background-color: #f8f8f8; padding: 1em; border-radius: 8px;">
                "Balance speeds up the population mean by a factor of \(b\)."
            </div>
            
            <p>
            
            So far, we've seen the spectral fingerprint: the rank-one mean term adds a single eigenvalue at \(-b\), while the random bulk remains a circular cloud of radius \(g\). This lone outlier is the ghost steering the mean activity. 
            But spectra are only the beginning. The NeurIPS paper showed how this outlier governs the <i>dynamical timescale</i>  of the mean. Take a step input and watch the population mean \(m(t)\) relax. The prediction from the balance equation is relatively simple:
            \[
            \tau_{\mathrm{eff}} = \frac{\tau}{b}.
            \]
            As \(b\) increases, the mean mode tracks input \(b\)-times faster. In simulations, fitting the exponential decay of \(m(t)\) confirms that the timescale drops as \(1/b\). 

            <div style="text-align: center; margin: 2em 0;">
                <img src="../assets/post3/fig1.png" alt="Balance-induced speedup of population dynamics" style="max-width: 50%; height: auto; border: 1px solid #ddd;">
                <p style="font-style: italic; color: #666; font-size: 0.9em; margin-top: 0.5em; max-width: 600px; margin-left: auto; margin-right: auto;">
                    <strong>Figure 1:</strong> Balance-induced speedup of population dynamics. The effective timescale τ<sub>eff</sub> of the population mean decreases as 1/b, demonstrating how tighter balance leads to faster tracking of external inputs.
                </p>
            </div>
            
            Faster mean dynamics suggest wider bandwidth for signal encoding. The DMFT closure makes this intuition quantitative. By treading residuals \(\tilde{h}(t)\) as a Gaussian process with covariance \(c(t, s)\), one can compute the input-output <i>coherence</i>
            \[
            C_{I\nu}(f) = \frac{\left| S_{I\nu}(f)\right|^2}{S_{II}(f) S_{\nu\nu}(f)},
            \]
            Here, \(S_{I\nu}\) is the cross-spectrum of the input and output, and \(S_{II}\), \(S_{\nu\nu}\) are their respective autospectra. The result is that higher balance values keep cohere close to one across higher frequencies, reflecting faster tracking and higher effective bandwidth

            <div style="display: flex; align-items: flex-start; margin: 2em 0; gap: 2em;">
                <img src="../assets/post3/fig2.png" alt="Input-output coherence as a function of frequency and balance" style="width: 45%; height: auto; border: 1px solid #ddd; flex-shrink: 0;">
                <div style="flex: 1;">
                    <p style="font-style: italic; color: #666; font-size: 0.9em; margin: 0 0 1em 0; line-height: 1.6;">
                        <strong>Figure 2:</strong> Input-output coherence as a function of frequency and balance. Higher balance values maintain coherence close to one across higher frequencies, demonstrating improved signal tracking and increased effective bandwidth.
                    </p>
                    <p style="margin: 0; line-height: 1.6;">
                        Integrating this coherence function gives us a Gaussian-channel lower bound on the <i>mutual information</i> between the input and output signals.
                        \[
                        R_{\mathrm{lb}} = -\int\, df \log_2\left(1 - C_{I\nu}(f)\right).
                        \]
                    </p>
                </div>
            </div>

            This is the one of the headline results of the paper, and our second core finding.
            
            <div style="text-align: center; margin: 0.5em 0 2em 0; background-color: #f8f8f8; padding: 1em; border-radius: 8px;">
                "The information rate grows nearly linearly with balance until it saturates at the bandwidth limit imposed by the input itself."
            </div>

            <div style="display: flex; align-items: flex-start; margin: 2em 0; gap: 2em;">
                <div style="flex: 1;">
                    <p style="font-style: italic; color: #666; font-size: 0.9em; margin: 0 0 1em 0; line-height: 1.6;">
                        <strong>Figure 3:</strong> Information rate scaling with balance parameter. The mutual information between input and output grows nearly linearly with balance until saturating at the bandwidth limit of the input signal.
                    </p>
                    <p style="margin: 0; line-height: 1.6;">
                        To complete our understanding of this paper, it's worth highlighting a key result from <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010590">a complementary paper</a> done by Engelken et al. This PLOS paper provides code that shows the same non-stationary DMFT framework was used to compute the largest Lyapunov exponent \(\lambda_1\).
                    </p>
                </div>
                <img src="../assets/post3/fig3.png" alt="Information rate scaling with balance parameter" style="width: 45%; height: auto; border: 1px solid #ddd; flex-shrink: 0;">
            </div>

            The surprising finding was that common input is poor at surpressing chaos. Because of balance, the common drive enters the mean equation and is canceled by recurrent inhibition. As a result, \(\lambda_1\) only turns negative at very large input amplitudes. Input independent, on the other hand, enters the fluctuation channel directly and is not canceled, so even small amplitudes can suppress chaos.

            <div style="text-align: center; margin: 2em 0;">
                <img src="../assets/post3/fig4.png" alt="Lyapunov exponent suppression by common vs independent input" style="max-width: 50%; height: auto; border: 1px solid #ddd;">
                <p style="font-style: italic; color: #666; font-size: 0.9em; margin-top: 0.5em; max-width: 600px; margin-left: auto; margin-right: auto;">
                    <strong>Figure 4:</strong> Chaos suppression by input type. Common input (top) requires large amplitudes to suppress chaos due to balance cancellation, while independent input (bottom) can suppress chaos even at small amplitudes by directly affecting fluctuation dynamics.
                </p>
            </div>
            
            This closes the loop on what we know. Balance plants a real outlier eigenvalue at \(-b\), accelerates the mean mode, broadens encoding bandwidth, and shapes how chaos can be tamed. In the next post, I'll go beyond balance and introduce low-rank perturbations
            \[
            S = \sum_{k=1}^R m_k u_k v_k^T,
            \]
            on top of the random bulk. Each term creates a new spectral outlier \(\lambda_{\mathrm{out}}\). My goal is to show that the real part of these outliers predicts exactly when the network bifurcates (from a fixed point to Hopf oscillations to chaos.) The hope is to expend DMFT with low-rank order parameters and make the ghost in the machine fully quantitative.
            <p>

    </p>
</div>

<script>
    function toggleProof(proofId) {
        const proof = document.getElementById(proofId);
        proof.classList.toggle('collapsed');
    }
</script>

</body>
</html>
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Vollkorn:wght@400;600&display=swap" rel="stylesheet">
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="style.css">
  <title>research</title>
</head>

<body>
  <div id="contentContainer">

    <div class="top-link">
        <a href="about.html">&larr; back</a>
    </div>

    <h2 class="katex-title">Research</h2>
    <hr>

    <div class="profile-section">
      <!-- <img src="assets/ad.png" alt="Profile Photo" class="profile-photo"> -->

      <div class="bio-text">
        <p> <b><a href="https://lpna.cs.illinois.edu/" class="katex-title" style="font-size:inherit; font-weight:500;" target="_blank">Lab for Parallel Numerical Algorithms</a></b> <br>
            Currently, I am studying probabilistic methods for estimating the contraction of closed tensor networks. We are developing a Markov chain Monte Carlo algorithm, borrowing methods from statistical physics, to estimate contractions such as the trace of matrix products.
        </p>
        <p>
            For example, consider the following tensor contraction
            \[
                \text{Tr}(ABCD) = \sum_{ijkl} A_{ij}B_{jk}C_{kl}D_{li}
            \]
            This is a closed ring network (all indices are contracted). The direct computation requires multiple matrix multiplications, an \(\mathcal{O}(n^3)\) operation. However, we can use a Markov chain to sample the indices \(i,j,k,l\) and estimate the trace. To improve mixing, we borrow some clever sampling techniques from statistical physics.
        </p>
        <p>
          While this is a somewhat trivial (direct multiplication is often faster in this case), finding an optimal contraction order for larger networks is NP-hard. High state-space systems, such as those arising in quantum chemistry and quantum circuits, would benefit greatly from effective contraction approximation algorithms.
        </p>
        More updates soon!
      </div>
      

      <div class="bio-text">
        <p> 
            <b><a href="https://conelab.beckman.illinois.edu/" class="katex-title" style="font-size:inherit; font-weight:500;" target="_blank">Computation & Neurodynamics Lab</a></b> <br>
            I am currently studying the dynamics of stochastic neuronal populations using moment closure methods. Starting from a nonlinear stochastic differential equation (SDE) that governs individual neuron behavior, we analytically derive a reduced system of ordinary differential equations (ODEs) that describe the evolution of the population mean and covariance over time. This involves evaluating Jacobians, noise terms, and closure assumptions that approximate the effect of higher-order cumulants.
        </p>
        <p>
            The system under consideration takes the form
            \[
                dx = f(x, \lambda) dt + G(x, \lambda) dW_t
            \]
            where \(x(t)\) is the neuronal state vector (e.g., membrane voltage and recovery variable), \(\lambda\) represents static input heterogeneity across the population, and \(W_t\) is vector-valued white noise. Nonlinearities in \(f\) (e.g., from cubic terms or feedback loops) make exact inference intractable, but we leverage a moment-based expansion to evolve just the first two moments, which are often sufficient for capturing macro-scale system behavior.
        </p>
        <p>
            The symbolic derivation of the moment ODEs is done using <code style="font-size:0.92em;">sympy</code>, and validated against Monte Carlo simulations of thousands of stochastic neurons. Our next step is to build surrogate models—such as neural networks or Gaussian processes—that can learn the mapping from inputs \((\lambda, t)\) to outputs \((\mu(t), \Sigma(t))\). These models enable fast approximations of neural behavior and could be used in optimal control, parameter inference, and theoretical neuroscience pipelines.
        </p>
        <p>
            Recently, I’ve been exploring extensions of this framework toward geometric and manifold-based learning, particularly for systems with nonlinear latent structure. Parallel to this, I’m beginning to investigate how similar stochastic and reduced-order techniques can be applied in quantum systems, including quantum channels and open quantum dynamics.
        </p>
        <p>
            Current experiments include benchmarking surrogate architectures (e.g., feedforward nets vs. GPR), integrating PCA-based dimensionality reduction, and extending the framework to richer models such as FitzHugh–Nagumo. The full analysis and implementation is available on my <a href="https://github.com/aniket-desh/fitzhugh-nagumo" class="katex-title" style="font-size:inherit; font-weight:500;" target="_blank">GitHub</a>.
        </p>
        More updates soon!
      </div>
    </div>


    <!-- <h2>Other</h2>

    <p>
        Materials from past applications and proposals can be found <a href="#">here</a>.
    </p> -->

    <div class="menu">
      <p>
        <a href="index.html">home</a> /
        <a href="notes.html">notes</a> /
        <a href="blog.html">blog</a> /
        <a href="../cv/cv.pdf">vitae</a>
      </p>
    </div>

  </div>
</body>

</html>
